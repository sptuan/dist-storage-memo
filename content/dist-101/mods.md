---
title: "4.5 分布式存储形态与组件"
weight: 5
---

{{< callout type="info" >}}
✋🏻😭✋🏻 本小节编辑中 ✍️✍️✍️
{{< /callout >}}


## 前言

将规律抽象出来是困难的。尤其是超越了不同类型的分布式系统(数据库/存储/消息队列)，在上层总结出的理论和经验。也是我们初看《DDIA》这本书时一头雾水的原因 —— **一旦超出了我们认知的具体用例，仿佛读到了什么，又仿佛什么都没读到**。我们一定事先明确研究内容和背景，才能进一步讨论具体的设计思路、技术手法。

因此本节将重点讨论：

1. 分布式存储系统的产品形态（对象存储、块存储、文件存储等）
2. 分布式存储系统实现中的常见组件(存储引擎、元数据以及管理节点等)


其他技术背景(比如分布式数据库)的读者，也可将本节作为综述，了解分布式存储工程师的视角。

## 分布式存储系统形态

随着大规模的使用和云服务的发展，自然形成了不同的产品形态，可分为对象存储、块存储、文件系统存储。

### 对象存储

提到对象存储，我们一般认为 S3 已经成为了一种事实标准，任何一个提供对象存储语义的服务都不太可能绕过该用户接口。S3 的全称 **S**imple **S**torage **S**ervice，这个 Simple 到底 Simple 在哪里？

考虑一个大型互联网公司存储用户媒体数据的需求，这个数据可能有 1EiB 之巨。则此用户适合使用对象存储的理由如下：


**Simple: 线性扩展和性能要求**

- 集群存储 1TiB 和 1EiB，读写延迟数量级大致不变，都在百毫秒
- 集群存储 1TiB 和 1EiB，读写 iops 和带宽随着数据量线性增加
- 集群存储 1TiB 到 1 EiB，扩缩容只需要提供机器资源即可，反之缩容即可

这种业务一般不会一根筋去死抠几百毫秒的延迟，但对集群的吞吐要求高，要求随着机器资源线性增长。存储研发人员应重点保证系统性能的线性扩展。

**Simple: 操作语义简单**

这个 “语义简单” 是针对 Posix 文件系统的语义来讲的。此类业务一般对 Posix 的原子性放松很多 —— 重点支持 PUT/DEL 这种操作。

另外，在对象存储中，对象名称是平铺的，使用 "/" 来模拟文件夹语义，实际上就是 k-v 语义，将用户的一整个对象名看做一个 key。用户使用文件夹语义做 ls 操作是根据 key prefix 模拟出来的。

比如对象所在 bucket: `b1`, 对象名: `dir1/dir2/dir3/1.jpg`。简单的 S3 对象存储会直接将 `dir1/dir2/dir3/1.jpg` 看做一整个 key。

**Simple: 牺牲了文件夹语义(以及性能)**

这种平铺的用户空间设计带来很好的扩展性和性能。代价是需要业务不能重度依赖 dir 语义，比如重度的 ls、rename、mv。而相当一些操作是不保证原子性的。

如用户使用 s3cmd 对对 dir 进行 `du` 和 `ls`，原理是进行 key-prefix 以模拟文件夹语义。在性能和计算消耗上，这和原生支持文件树数据结构的分布式文件存储系统无法相提并论。

这也是经常能看到由厂商将 S3 操作分为 A 类和 B 类，而定价不同[^cloudflare_r2_pricing]的有趣原因。

[^cloudflare_r2_pricing]: [Cloudflare R2 - Pricing](https://developers.cloudflare.com/r2/pricing/#class-a-operations)

**Simple: 不支持覆盖写**

当一个对象成功写入后，其就是 immutable 的。用户无法对其中一块 range 的对象做覆盖写，这也是区别于文件系统的重要特性。


**用户形态**

S3 对象存储最适合存储巨量媒体数据的业务，或在 AI 训练中大量读取、写入 checkpoint 的场景，或大量的备份存储。它们对集群的可扩展、吞吐要求极高，而对 io 时延无极限要求。

当用户有覆盖写、大量的原子性 copy/rename，强依赖于文件夹操作时，平坦的对象存储显然无法适应其要求。

（有厂商支持部分文件树语义的对象存储形态，不过笔者认为其更像是一种支持部分 posix 语义的文件存储了）。

另外，进一步考虑多数据中心场景下的多活写入一致性问题，对象存储往往使用简单的最终胜出一致策略，或者干脆保留多版本。不涉及类似 kv 数据库的复杂一致性保证。

### 块存储

### 文件存储


## 元数据和存储引擎

元数据和存储引擎的分层设计，常见于对象存储和文件存储产品形态中。

大型分布式对象/文件集群需要动辄支撑千亿、万亿级别的元数据，一个重要的流派就是元数据管理逐渐独立出来，作为并行于存储引擎的组件。甚至直接使用 NoSQL 数据库，以得到近似线性的扩容支撑能力。

有综述论文[^meta_overview]描述了元数据系统的发展里程碑：

[^meta_overview]: H. Dai, Y. Wang, K. B. Kent, L. Zeng and C. Xu, "[The State of the Art of Metadata Managements in Large-Scale Distributed File Systems — Scalability, Performance and Availability](https://ieeexplore.ieee.org/document/9768784)" in IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 12, pp. 3850-3869, 1 Dec. 2022, doi: 10.1109/TPDS.2022.3170574.

![](https://static.zdfmc.net/imgs/2025/11/0eae61fc3b510d9e.png)

而块存储主要为用户提供块设备的线性空间，元数据管理的量级较小，一般使用单组高可用共识节点即可管理。


{{< callout type="info" >}}
✋🏻😭✋🏻 本小节仍然在编辑中，请稍后回来 ✍️✍️✍️
{{< /callout >}}