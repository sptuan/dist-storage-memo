---
title: "4.3 中登必懂之分区与复制"
weight: 3
cascade:
  type: docs
---

{{< callout type="info" >}}
✋🏻😭✋🏻 本小节编辑中 ✍️✍️✍️
{{< /callout >}}

分布式人两个重要手段：Partition 和 Replicate。面对新人时，我更愿把它们称为 “伎俩”。实是因为其宛如两板斧一样大道至简，不必故弄玄虚。

然而，一旦分布式开发者熟悉了这套模式，就会惊奇地发现，所有的分布式系统方案，本质上都是在回答这两个问题（也会惊奇地发现自己由小登向中登转变，难免有一丝淡淡的悲伤✋🏻😭✋🏻）。


## 背景和动机
让我们暂时忘记这些手段吧，不妨先来看看技术背景是什么。前文中，我们讨论了分布式系统中混沌的环境，网络、计算随时都可能崩溃、损坏。

如果我们仍然延续单机应用的做法，会发生什么？

- **数据安全性威胁**：数据只有一份，先不考虑天灾，只是这块磁盘挂点后就再无退路可言！
- **数据单机瓶颈**：磁盘/内存的读写总是有上限的，那超过上限的请求，怎么抗？
- **计算单机瓶颈**：CPU 的计算能力和线程数量总是有上限的，那超过上限的请求，怎么抗？

如果我们有一台能够无限扩容的单机就好了！然而事实是残酷的。有个**最简单的方式是花钱请人设计系统[^fun]**！（这样我们就能把他们交付的系统当做单机用了！^_^ ）。要么只能想一些工程手段去绕开单机的瓶颈。

[^fun]: [Distributed systems for fun and profit](https://book.mixu.net/distsys/single-page.html)

分布式思想是如此地朴素：
1. 单机无法满足要求，那多台机器咋样？
2. 单机磁盘容易爆炸，那我就多复制几份可否？顺便顶一下磁盘 IO 流量？
3. 单机计算不行，那我们就分开计算、最后合并结果如何？

现有常见的分布式系统便可以如下梳理：
| 系统       | 复制（Replication）核心思想                                                                 | 分区（Partition）核心思想                                                                 |
|------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|
| **GFS** - 存储  | **数据副本**：（默认3份）跨机架存储数据块，实现容错和高可用，避免单点/机架 故障导致数据丢失       | **数据分区**：将大文件拆分为固定大小数据块（64MB），分散存储在不同节点，突破单机存储容量限制               |
| **MapReduce** - 计算 | **任务副本**：当某个 Reduce 任务失败时，系统会在其他节点重新调度该任务（计算任务的复制）     | **任务分区**：将输入数据分片，每个分片对应一个 Map 任务；中间结果按 Key 分区，实现并行计算和相同 Key 的聚合处理   |
| **Flink** - 计算     | **任务副本**：关键任务可配置 “备用任务（Standby Task）”，主任务故障时快速切换      | **任务分区**：将数据流拆分为多个并行子流，通过灵活的分区策略（哈希、轮询等）分配给不同算子实例，提升计算并行度 |
| **Kafka** - 消息队列     | **数据副本**：每个分区，Leader 负责读写，Follower 同步数据并作为故障备份         | **数据分区**：将主题拆分为多个分区，作为并行读写的基本单位，通过 Key 路由保证相同消息的有序性和集中处理         |

这些系统无非是将任务或者数据进行分区和复制操作。鉴于我们是分布式存储漫游指南，后续本文内容都是指存储系统视角。

## 复制链路和一致性模型

背景虚头巴脑的说完了，数据复制总要选取一种具体的方式。使用单一维度来归纳复制手段实在困难，本节纯属作者个人理解，从**复制组、复制链路、一致性达成条件**来归纳。

现在考虑一个最简单的 **三副本数据复制** 场景。

![](https://static.zdfmc.net/imgs/2025/10/1c000abbdb32c6fe.png)


### 分布式存储引擎与复制组

复制组是分布式存储系统中的一个基本存储单元。这个单元目的就是**将用户数据读写和内部的数据复制机制分层隔离**。

对于用户：

1. 带着自己的key和数据(或只携带数据)前来写入
2. 开开心心地拿到成功的 Response 继续干活
3. “不关心”[^rp]底层怎么达成的复制和一致性 

对于分布式存储引擎：

1. 输入是上层的一小块数据。
2. 输出是一个代表数据路由的 key。用户可以通过这个唯一 key 来取回数据。
3. 内部如何对数据复制、修复、甚至编码转换，都是对用户透明的[^rp]。

一个可能的分布式存储引擎系统设计例：

![](https://static.zdfmc.net/imgs/2025/10/1de54e75790141f4.png)

该例子中，将一组物理或虚拟的 volume 结成一组，编码类型为副本，数量为3。

设计这个单元需要考虑编码类型 (副本组/EC组)、容量规模(~64MiB or ~512GiB)。创建和删除形式中，它可能也是多样的 (静态逻辑卷/动态数据块)。各类系统（文件系统, 对象存储, 块存储等）根据需求设计合适的手段。以下使用表格总结公开大型系统的复制组设计。

组织形式，复制组规模，编码支持

### 链路：主节点复制

### 链路：链式复制

### 链路：星形复制

### 链路：代理模式

### 一致性：全一致性写

### 一致性：多数派写

### 一致性：任意数量写

### 复制与数据安全模型

### 复制与成本模型

## 分区

### 分区与路由

### 

## 继续阅读
- 几乎人人都有的书 《设计数据密集型应用》[^ddia] 的 “复制”章节详细描述了**更全面的复制**手段。 （不过根据作者实际体验，若不带着实际的问题，很难体会到一些手段的动机、思想和目的。）


[^ddia]: [Designing Data-Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/)

[^rp]: 实际上层业务也要关心存储底层的原理。要设计可用性、容错策略、存储成本和性能皆佳的系统，上层使用者需要根据底层的优劣势联合设计读写流程。说 “不关心” 只是为了强调模块的分层设计。

