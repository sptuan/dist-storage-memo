---
title: "数据编码，性能-成本-可靠性魔法"
weight: 20
---
{{< callout type="info" >}}
✋🏻😭✋🏻 本小节编辑中 ✍️✍️✍️
{{< /callout >}}

## 成本优先与精益存储

分布式存储的成本始终是用户关注的重点之一。企业和用户在当今注重精益运营、降本增效的背景下，也越来越重视单位数据的存储成本。

比如，在所有需要持久存储的互联网媒体数据中，大概也符合 28 定律，即 80% 的数据只占有了 20% 的访问量[^fb_data_rate]。再比如，某用户的读写模型都是纯写、几乎不读（合规存档目的）。针对这些数据特征，是不是仍有降低成本的空间？都是非常有实际意义的研究课题。

近年来，在实际业务需求中探索存储成本，在满足性能需求前提下制定合适的编码策略，也成为了分布式存储开发者必备技能之一。

## 数据可靠魔法
### 复制：多个篮子，得加钱

保证数据可靠安全的秘诀是什么？非常简单，就是复制多份出去。正如老话所讲的 “鸡蛋不能放到同一个篮子中”。

既然单个数据可能随着硬盘爆炸、机器断电导致数据丢失，那只能是复制多份。但凡一份数据损坏，立即启动修复，趁着其他份还健康的时候赶紧恢复回来。

![](https://static.zdfmc.net/imgs/2025/12/2ee88f7e1b6a870b8f78077153cea1df.png)

用户对于多份复制带来的成本是惊诧的。多份复制要有多份的写流量。后面提到的不同编码方式中，还可能影响到系统元数据的多次读写、用户读写性能的降低。但这些行为对大部分用户是透明的。


### 故障域：篮子不能放在同一辆板车上

单纯地多份复制不一定奏效，考虑同一个机房的同一个机架。这个机架上的两台机器，在火灾、断电、甚至仅仅是某台路由器故障时，大概率是“同生共死”的。如果多个副本放在同一个机架上，用户因数据丢失而红温的概率无疑会大大提高。因此，需要引入更上层的 “篮子划分”。

因此存储系统将故障域这个概念建立起来。**根据故障域分配合适的数据复制存储位置，是现代分布式存储的必须能力之一**。

在 Google 发布的关于分布式存储系统可用性论文[^google_ava]中，对故障级别进行如下的归类。

[^google_ava]:[Ford, Daniel, et al. "Availability in globally distributed storage systems." 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI 10). 2010.](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Ford.pdf)

| 故障范围   | 基本概念                                                                 | 抵御故障手法                                                                 |
|------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Disk（磁盘）** | 存储数据的基础硬件组件，负责永久存储数据，故障可能导致数据校验不匹配或永久性丢失 | 1. 背景清理进程（检测并处理校验不匹配的数据块）；2. 客户端读取时验证数据完整性；3. 数据复制/纠删编码（Reed-Solomon）冗余备份 |
| **Node（节点）** | 运行存储服务器程序的物理机器，管理本地磁盘存储，故障多为暂时性（软件/硬件/网络问题等） | 1. 数据多副本或纠删编码分布在不同节点；2. 15分钟延迟恢复机制（避免 transient 故障无效恢复）；3. 集群负载均衡与节点状态监控 |
| **Rack（机架）** | 集中部署的一组存储节点集合，共享网络交换机、电源电缆等基础设施，是核心故障域 | 1. 机架感知放置策略（同一数据条带的块不部署在同一机架）；2. 跨机架数据复制；3. 独立机架电源/网络冗余设计 |
| **Cell（存储单元）** | 由数千个节点及上层协调进程组成的独立存储集群，通常部署在单一建筑/相邻建筑群 | 1. 多副本（R=2/3/4等）或纠删编码（RS(n,m)）方案；2. 基于Markov模型的故障状态管理与优先级恢复；3. 故障爆发检测与批量恢复队列优化 |
| **Datacenter（数据中心）** | 存储Cell的物理部署载体，包含多个Cell，共享机房级电源、网络等基础设施 | 1. 多Cell跨数据中心复制（如R=3×2、RS(6,3)×3）；2. 数据中心间独立基础设施（无共享故障点）；3. 跨中心恢复带宽优化与按需数据恢复 |


该文章的年代较早(2010s)，但仍然具有很大的参考意义。这种等级划分和实际基建设施的架构有关，比如有的云厂商提供的多可用区(AZ)[^qcloud_cos_az]概念。核心的思路就是需要实际情况，在数据副本放置策略上尽量避开一损俱损。

[^qcloud_cos_az]:[对象存储多 AZ 特性概述 - 腾讯云](https://cloud.tencent.com/document/product/436/40548)

值得指出的是，较远物理距离的复制（比如数据中心之间）天然带来了读写延迟增高的代价。存储开发者需要帮助用户做出合理选择。

#### 例：经典的两地三中心架构

比如，当考虑到多个数据中心 + Rack 时候，一个可能的数据复制策略如下图所示。

![](https://static.zdfmc.net/imgs/2025/12/cb4e898ea6bc2cbbc916129ced9f70b7.png)

一个文件被复制成为了 3 副本，其中两份放在同一个数据中心的不同 Rack 上。一份通过异步备份，放在远端数据中心负责灾备。这就是最经典的**两地三中心**架构。


### 密度和性能：水池变大了，但水管没变

### 冷数据、旧机器与介质


## 编码、成本与性能模型

### 冗余度与信息论

### 副本

### EC 编码

### LRC 编码

## 数据安全性模型

### 平均磁盘年故障率

### 集群修复速度

### 编码与数据安全

## 动手做：为用户制定合适的编码策略

## 小结


